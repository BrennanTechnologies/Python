ST-MoE-32B
GitHub/Model URL:
Submitted: 3 Feb 2022
Score:
91.2
More details
Model Description
A sparsely activated Mixture-of-Expert model with 269B parameters, FLOP-matched to a 32B parameter dense model. Pre-trained on C4 corpus(Raffel et al., 2019).
Parameter Description
269b parameters roughly 11 % activated
Parameter Information
Shared w/o fine-tuning: -1
Fine-tuned: -1
Task Specific: -1
Diagnostics Information
Broadcoverage Diagnostics confusion matrix
NE
N56480
E69391
N = not entailment
E = entailment
Category-wise Matthew's Correlation Scores
Lexical Semantics: 77.0
Lexical Entailment: 69.6
Morphological Negation: 85.7
Factivity: 64.0
Symmetry/Collectivity: 100.0
Redundancy: 27.7
Named Entities: 88.9
Quantifiers: 74.7
Knowledge: 68.8
Common Sense: 69.8
World Knowledge: 67.1
Logic: 61.5
Negation: 78.2
Double Negation: 100.0
Interval/Numbers: 78.0
Conjuction: 94.9
Disjunction: 14.0
Conditionals: 48.8
Universal: 76.6
Existential: 56.0
Temporal: 50.6
Upward Monotone: 88.1
Downward Monotone: -22.3
NonMonotonic: 39.4
Predicate Argument Structure: 76.9
Core Args: 85.5
Prepositional Phrases: 88.4
Ellipsis/Implicits: 77.1
Anaphora/Coreference: 77.0
Active/Passive: 94.2
Nominalization: 100.0
Genitives/Partitives: 100.0
Datives: 88.2
Relative Clauses: 46.9
Coordination Scopes: 81.1
Intersectivity: 46.2
Restrictivity: 10.5
